version: '3.8'

# ==========================================
# ‚ö° AIRFLOW SHARED CONFIGURATION
# ==========================================
# This block defines settings used by all Airflow services (Webserver, Scheduler, Init)
# to avoid repetition and keep the file clean.
x-airflow-common: &airflow-common
  build:
    context: ../  # Build context is the project root
    dockerfile: infrastructure/airflow.Dockerfile
  environment:
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow
    - AIRFLOW_CONN_POSTGRES_DEFAULT=postgresql://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    # ---------------------------------------------------------
    # CRITICAL: Add the 'dags' folder to PYTHONPATH.
    # This allows python to import 'modules' and 'utils' packages
    # found in the mounted source directory.
    # ---------------------------------------------------------
    - PYTHONPATH=/opt/airflow/dags
  volumes:
    # 1. CODE: Mount 'src' to 'dags'. This maps your local code to the container.
    - ../src:/opt/airflow/dags
    
    # 2. CONFIG: Mount .env so the code can read secrets/URLs.
    - ./.env:/opt/airflow/.env
    
    # 3. LOGS: Persist execution logs.
    - airflow-logs:/opt/airflow/logs
    
    # Maps the root 'data' folder to '/opt/airflow/data' in container
    - ../data:/opt/airflow/data
  networks:
    - bdanet
  depends_on:
    - postgres
    - namenode

services:

  # ==========================================
  # üíæ STORAGE LAYER (Postgres & Hadoop)
  # ==========================================
  
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - bdanet
    ports:
      - "5432:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow" ]
      interval: 10s
      timeout: 5s
      retries: 5

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_replication=1
      - HADOOP_HEAPSIZE=200
    volumes:
      - hdfs-namenode:/hadoop/dfs/name
    networks:
      - bdanet
    ports:
      - "49000:50070"  # Web UI
      - "8020:8020"    # HDFS IPC

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_fs_s3a_aws_credentials_provider=org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider
      - CORE_CONF_fs_s3a_impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      - HADOOP_HEAPSIZE=200
    volumes:
      - hdfs-datanode:/hadoop/dfs/data
    networks:
      - bdanet

  # ==========================================
  # ‚öôÔ∏è ORCHESTRATION LAYER (Airflow)
  # ==========================================

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    restart: always
    ports:
      - "8080:8080"
    depends_on:
      - airflow-init

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always
    depends_on:
      - airflow-init

  airflow-init:
    <<: *airflow-common
    command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
    depends_on:
      - postgres

  # ==========================================
  # üß† SEMANTIC LAYER (GraphDB)
  # ==========================================

  graphdb:
    image: ontotext/graphdb:10.6.0
    container_name: graphdb
    environment:
      GDB_HEAP_SIZE: "2g"
    volumes:
      - graphdb-data:/opt/graphdb/home
      # Mount external statements file if needed
      - ../statements.rj:/opt/graphdb/home/graphdb-import/statements.rj
    networks:
      - bdanet
    ports:
      - "7200:7200"
      - "7300:7300"
    healthcheck:
      test: [ "CMD", "wget", "--spider", "http://localhost:7200/rest/repositories" ]
      interval: 10s
      timeout: 5s
      retries: 12
    restart: unless-stopped

  graphdb-init:
    image: python:3.9-slim
    container_name: graphdb-init
    networks:
      - bdanet
    volumes:
      # Map the python script and TTL configs from the 'modules/semantic' folder
      - ../src/modules/semantic/setup_graphdb.py:/app/setup_graphdb.py
      - ../src/modules/semantic/moviekg-config.ttl:/app/moviekg-config.ttl
      - ../src/modules/semantic/graph_model.ttl:/app/graph_model.ttl
    working_dir: /app
    command: >
      bash -c "pip install requests && python setup_graphdb.py"
    depends_on:
      graphdb:
        condition: service_healthy

# ==========================================
# üì¶ VOLUMES & NETWORKS
# ==========================================
volumes:
  postgres-data:
  hdfs-namenode:
  hdfs-datanode:
  airflow-logs:
  graphdb-data:

networks:
  bdanet: